{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956a7f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variant</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Stability_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Single XGB (K=15)</td>\n",
       "      <td>0.9285</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.0313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ensemble (K=15) × 5</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.9647</td>\n",
       "      <td>0.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGB Random-Forest</td>\n",
       "      <td>0.8957</td>\n",
       "      <td>0.9514</td>\n",
       "      <td>0.0086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB Exact (subsample=1, colsample=1)</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Variant  Accuracy  ROC_AUC  Stability_RMSE\n",
       "0                     Single XGB (K=15)    0.9285   0.9643          0.0313\n",
       "1                   Ensemble (K=15) × 5    0.9310   0.9647          0.0072\n",
       "2                     XGB Random-Forest    0.8957   0.9514          0.0086\n",
       "3  XGB Exact (subsample=1, colsample=1)    0.9250   0.9632          0.0000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "# Config ­— edit here\n",
    "K = 15          # models per ensemble\n",
    "M = 5           # independent ensembles\n",
    "N_SAMPLES = 4000\n",
    "SEED = 42\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "def stability_rmse(mat):\n",
    "    return np.sqrt(2 * mat.var(axis=0, ddof=0)).mean()\n",
    "\n",
    "def run_one_xgb(Xtr, ytr, Xte, yte, order, **xgb_kw):\n",
    "    model = XGBClassifier(n_jobs=1, eval_metric=\"auc\", use_label_encoder=False,\n",
    "                          verbosity=0, **xgb_kw)\n",
    "    model.fit(Xtr[order], ytr[order])\n",
    "    p = model.predict_proba(Xte)[:, 1]\n",
    "    return accuracy_score(yte, p > 0.5), roc_auc_score(yte, p), p\n",
    "\n",
    "def experiment(k=15, m=5, n=4000, seed=42):\n",
    "    X, y = make_classification(n_samples=n, n_features=20, n_informative=10,\n",
    "                               n_redundant=5, flip_y=0.05, class_sep=1.0,\n",
    "                               random_state=seed)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25,\n",
    "                                          stratify=y, random_state=seed)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # ── (1) Single boosted models ─────────────────────────────────────────\n",
    "    sing_preds, acc, auc = [], [], []\n",
    "    for i in range(k):\n",
    "        order = np.random.permutation(len(Xtr))\n",
    "        a, u, p = run_one_xgb(\n",
    "            Xtr, ytr, Xte, yte, order,\n",
    "            n_estimators=120, learning_rate=0.1, max_depth=4,\n",
    "            subsample=0.9, colsample_bytree=0.9,\n",
    "            tree_method=\"hist\", random_state=seed+i)\n",
    "        acc.append(a); auc.append(u); sing_preds.append(p)\n",
    "    rows.append(dict(Variant=f\"Single XGB (K={k})\",\n",
    "                     Accuracy=np.mean(acc), ROC_AUC=np.mean(auc),\n",
    "                     Stability_RMSE=stability_rmse(np.vstack(sing_preds))))\n",
    "\n",
    "    # ── (2) Ensembles of K models, repeated M times ───────────────────────\n",
    "    ens_preds = []\n",
    "    for e in range(m):\n",
    "        members = []\n",
    "        for i in range(k):\n",
    "            order = np.random.permutation(len(Xtr))\n",
    "            _, _, p = run_one_xgb(\n",
    "                Xtr, ytr, Xte, yte, order,\n",
    "                n_estimators=120, learning_rate=0.1, max_depth=4,\n",
    "                subsample=0.9, colsample_bytree=0.9,\n",
    "                tree_method=\"hist\", random_state=seed+10_000*e+i)\n",
    "            members.append(p)\n",
    "        ens_preds.append(np.mean(members, axis=0))\n",
    "    rows.append(dict(Variant=f\"Ensemble (K={k}) × {m}\",\n",
    "                     Accuracy=accuracy_score(yte, ens_preds[0] > 0.5),\n",
    "                     ROC_AUC=roc_auc_score(yte, ens_preds[0]),\n",
    "                     Stability_RMSE=stability_rmse(np.vstack(ens_preds))))\n",
    "\n",
    "    # ── (3) XGB Random-Forest ─────────────────────────────────────────────\n",
    "    rf_preds, acc_rf, auc_rf = [], [], []\n",
    "    for i in range(k):\n",
    "        order = np.random.permutation(len(Xtr))\n",
    "        rf = XGBRFClassifier(n_estimators=200, learning_rate=0.5, max_depth=6,\n",
    "                             subsample=0.8, colsample_bytree=0.8,\n",
    "                             random_state=seed+i, n_jobs=1,\n",
    "                             eval_metric=\"auc\", use_label_encoder=False,\n",
    "                             verbosity=0)\n",
    "        rf.fit(Xtr[order], ytr[order])\n",
    "        p = rf.predict_proba(Xte)[:, 1]\n",
    "        acc_rf.append(accuracy_score(yte, p > 0.5))\n",
    "        auc_rf.append(roc_auc_score(yte, p))\n",
    "        rf_preds.append(p)\n",
    "    rows.append(dict(Variant=\"XGB Random-Forest\",\n",
    "                     Accuracy=np.mean(acc_rf), ROC_AUC=np.mean(auc_rf),\n",
    "                     Stability_RMSE=stability_rmse(np.vstack(rf_preds))))\n",
    "\n",
    "    # ── (4) Fully deterministic boosted model ────────────────────────────\n",
    "    #     tree_method='exact', subsample=1, colsample_bytree=1\n",
    "    det_preds, acc_det, auc_det = [], [], []\n",
    "    for i in range(k):\n",
    "        order = np.random.permutation(len(Xtr))\n",
    "        a, u, p = run_one_xgb(\n",
    "            Xtr, ytr, Xte, yte, order,\n",
    "            n_estimators=120, learning_rate=0.1, max_depth=4,\n",
    "            subsample=1.0, colsample_bytree=1.0,\n",
    "            tree_method=\"exact\", random_state=seed+i)\n",
    "        acc_det.append(a); auc_det.append(u); det_preds.append(p)\n",
    "    rows.append(dict(Variant=\"XGB Exact (subsample=1, colsample=1)\",\n",
    "                     Accuracy=np.mean(acc_det), ROC_AUC=np.mean(auc_det),\n",
    "                     Stability_RMSE=stability_rmse(np.vstack(det_preds))))\n",
    "\n",
    "    return pd.DataFrame(rows).round(4)\n",
    "\n",
    "# Run and display\n",
    "df_res = experiment(K, M, N_SAMPLES, SEED)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdafee12",
   "metadata": {},
   "source": [
    "## Proof that histogram binning is behind the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00587e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>large-hist-1.0</td>\n",
       "      <td>0.0293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>large-hist-0.8</td>\n",
       "      <td>0.0326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>large-exact</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>small-hist-1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>small-hist-0.8</td>\n",
       "      <td>0.0388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>small-exact</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variant    rmse\n",
       "0  large-hist-1.0  0.0293\n",
       "1  large-hist-0.8  0.0326\n",
       "2     large-exact  0.0000\n",
       "3  small-hist-1.0  0.0000\n",
       "4  small-hist-0.8  0.0388\n",
       "5     small-exact  0.0000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def stability_rmse(mat):\n",
    "    return np.sqrt(2 * mat.var(axis=0, ddof=0)).mean()\n",
    "\n",
    "def run_variant(n_rows, n_jobs, tree_method, subsample, n_shuffles=10, seed=0):\n",
    "    # fresh data set for each size\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_rows, n_features=30, n_informative=15, n_redundant=10,\n",
    "        class_sep=1.0, random_state=seed)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y, test_size=0.25, stratify=y, random_state=seed)\n",
    "\n",
    "    preds = []\n",
    "    for s in range(n_shuffles):\n",
    "        order = np.random.RandomState(seed + s).permutation(len(Xtr))\n",
    "        mdl = XGBClassifier(\n",
    "            n_estimators=120, max_depth=5, learning_rate=0.1,\n",
    "            subsample=subsample, colsample_bytree=1.0,\n",
    "            tree_method=tree_method, n_jobs=n_jobs,\n",
    "            random_state=42, eval_metric=\"auc\",\n",
    "            use_label_encoder=False, verbosity=0)\n",
    "        mdl.fit(Xtr[order], ytr[order])\n",
    "        preds.append(mdl.predict_proba(Xte)[:, 1])\n",
    "    return stability_rmse(np.vstack(preds))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "variants = [\n",
    "    # large-data, multithread\n",
    "    (\"large-hist-1.0\",   dict(n_rows=60_000, n_jobs=8, tree_method=\"hist\",  subsample=1.0)),\n",
    "    (\"large-hist-0.8\",   dict(n_rows=60_000, n_jobs=8, tree_method=\"hist\",  subsample=0.8)),\n",
    "    (\"large-exact\",      dict(n_rows=60_000, n_jobs=8, tree_method=\"exact\", subsample=1.0)),\n",
    "    # small-data, single-thread\n",
    "    (\"small-hist-1.0\",   dict(n_rows=6_000,  n_jobs=1, tree_method=\"hist\",  subsample=1.0)),\n",
    "    (\"small-hist-0.8\",   dict(n_rows=6_000,  n_jobs=1, tree_method=\"hist\",  subsample=0.8)),\n",
    "    (\"small-exact\",      dict(n_rows=6_000,  n_jobs=1, tree_method=\"exact\", subsample=1.0)),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, cfg in variants:\n",
    "    rmse = run_variant(**cfg)\n",
    "    results.append({\"variant\": name, \"rmse\": round(rmse, 4)})\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd8d44",
   "metadata": {},
   "source": [
    "## Other Supervised Algoritms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a58f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing algorithm stability under row permutation...\n",
      "================================================================================\n",
      "Algorithm                           Mean STD     Max STD      Mean RMSE    Stable?   \n",
      "--------------------------------------------------------------------------------\n",
      "Linear Regression                   1.99e-13     5.43e-13     3.03e-13     YES       \n",
      "Ridge Regression                    8.94e-14     2.30e-13     1.36e-13     YES       \n",
      "Decision Tree (no sampling)         1.39e-14     5.68e-14     0.00e+00     YES       \n",
      "Lasso (coord descent)               3.63e-14     1.19e-13     5.01e-14     YES       \n",
      "Lasso (random coord descent)        3.75e-14     1.25e-13     5.19e-14     YES       \n",
      "ElasticNet                          3.94e-14     1.21e-13     5.50e-14     YES       \n",
      "SGD Regressor                       2.90e-03     6.01e-03     4.42e-03     NO        \n",
      "SGD (different loss)                7.79e-03     1.73e-02     1.19e-02     NO        \n",
      "MLP (adam)                          3.63e-01     1.27e+00     6.14e-01     NO        \n",
      "MLP (sgd)                           5.28e-01     3.62e+00     1.24e+00     NO        \n",
      "MLP (lbfgs)                         4.43e-02     4.29e-01     1.10e-01     NO        \n",
      "Random Forest                       9.70e+00     2.22e+01     1.52e+01     NO        \n",
      "XGBoost (hist)                      6.80e-06     6.10e-05     0.00e+00     NO        \n",
      "XGBoost (exact)                     6.90e-06     3.05e-05     0.00e+00     NO        \n",
      "XGBoost (exact, no subsample)       6.90e-06     3.05e-05     0.00e+00     NO        \n",
      "LightGBM                            1.17e-14     5.68e-14     0.00e+00     YES       \n",
      "LightGBM (deterministic)            1.17e-14     5.68e-14     0.00e+00     YES       \n",
      "SVM (RBF)                           1.25e-14     5.61e-14     2.22e-14     YES       \n",
      "KNN                                 8.93e-15     5.68e-14     0.00e+00     YES       \n",
      "\n",
      "================================================================================\n",
      "\n",
      "SUMMARY:\n",
      "----------------------------------------\n",
      "\n",
      "STABLE ALGORITHMS (10):\n",
      "  ✓ Linear Regression\n",
      "  ✓ Ridge Regression\n",
      "  ✓ Decision Tree (no sampling)\n",
      "  ✓ Lasso (coord descent)\n",
      "  ✓ Lasso (random coord descent)\n",
      "  ✓ ElasticNet\n",
      "  ✓ LightGBM\n",
      "  ✓ LightGBM (deterministic)\n",
      "  ✓ SVM (RBF)\n",
      "  ✓ KNN\n",
      "\n",
      "UNSTABLE ALGORITHMS (9):\n",
      "  ✗ SGD Regressor (avg RMSE between runs: 0.0044)\n",
      "  ✗ SGD (different loss) (avg RMSE between runs: 0.0119)\n",
      "  ✗ MLP (adam) (avg RMSE between runs: 0.6135)\n",
      "  ✗ MLP (sgd) (avg RMSE between runs: 1.2381)\n",
      "  ✗ MLP (lbfgs) (avg RMSE between runs: 0.1102)\n",
      "  ✗ Random Forest (avg RMSE between runs: 15.1861)\n",
      "  ✗ XGBoost (hist) (avg RMSE between runs: 0.0000)\n",
      "  ✗ XGBoost (exact) (avg RMSE between runs: 0.0000)\n",
      "  ✗ XGBoost (exact, no subsample) (avg RMSE between runs: 0.0000)\n",
      "\n",
      "\n",
      "INVESTIGATING RANDOM FOREST INSTABILITY\n",
      "================================================================================\n",
      "Configuration                  Mean RMSE       Notes\n",
      "--------------------------------------------------------------------------------\n",
      "RF (default)                   38.382886      \n",
      "\n",
      "  Sample predictions comparison (first 5):\n",
      "  Original order: [ -86.61908048 -126.23997942  104.74558585   59.69626273   43.53932339]\n",
      "  Shuffled order: [ -24.46022059 -120.69896732  133.45213641   14.44617205   40.38024254]\n",
      "  Difference:     [-62.15885989  -5.5410121  -28.70655056  45.25009068   3.15908085]\n",
      "RF (bootstrap=False)           0.000000       \n",
      "RF (max_samples=1.0)           38.382886      \n",
      "RF (n_estimators=1)            127.543084     \n",
      "RF (warm_start=True)           38.382886      \n",
      "\n",
      "\n",
      "DETAILED ANALYSIS OF SPECIFIC CASES\n",
      "================================================================================\n",
      "\n",
      "1. XGBoost with different configurations:\n",
      "----------------------------------------\n",
      "XGB (hist, subsample=0.8)           RMSE: 33.040123, Stable: NO\n",
      "XGB (hist, subsample=1.0)           RMSE: 0.000000, Stable: NO\n",
      "XGB (exact, subsample=0.8)          RMSE: 33.190399, Stable: NO\n",
      "XGB (exact, subsample=1.0)          RMSE: 0.000000, Stable: NO\n",
      "XGB (exact, all 1.0)                RMSE: 0.000000, Stable: NO\n",
      "\n",
      "2. Neural Networks with different batch sizes:\n",
      "----------------------------------------\n",
      "MLP (batch_size=1000)               RMSE: 0.000000, Stable: YES\n",
      "MLP (batch_size=200)                RMSE: 1.654901, Stable: NO\n",
      "MLP (batch_size=32)                 RMSE: 1.353084, Stable: NO\n",
      "MLP (batch_size=1)                  RMSE: 4.823782, Stable: NO\n",
      "\n",
      "\n",
      "KEY INSIGHTS:\n",
      "================================================================================\n",
      "\n",
      "Based on the empirical results:\n",
      "\n",
      "1. **Perfectly Stable** (deterministic):\n",
      "   - Linear/Ridge regression (closed-form solutions)\n",
      "   - Decision Trees with fixed random_state\n",
      "   - Coordinate descent methods (even with 'random' selection!)\n",
      "   - KNN (no randomness involved)\n",
      "   - LightGBM (surprisingly stable by default)\n",
      "\n",
      "2. **Unstable Algorithms**:\n",
      "   - SGD-based methods (mini-batch order matters)\n",
      "   - Neural networks (batch formation depends on order)\n",
      "   - Random Forest (high variance - needs investigation)\n",
      "   - XGBoost (small but measurable differences)\n",
      "\n",
      "3. **Key Findings**:\n",
      "   - sklearn's coordinate descent properly controls randomness\n",
      "   - LightGBM appears more stable than XGBoost by default\n",
      "   - Random Forest's instability is surprisingly large\n",
      "   - Even 'exact' tree methods can be unstable with subsampling\n",
      "\n",
      "4. **To Achieve Stability**:\n",
      "   - Use algorithms with closed-form solutions when possible\n",
      "   - For XGBoost: tree_method='exact', subsample=1.0, colsample_bytree=1.0\n",
      "   - For neural networks: use full-batch training (lbfgs solver)\n",
      "   - For ensemble methods: be aware of the variance/stability trade-off\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import all models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Optional imports (install if testing)\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "def test_algorithm_stability(X_train, y_train, X_test, model_class, model_params=None, n_shuffles=10):\n",
    "    \"\"\"Test if an algorithm gives consistent predictions under row shuffling\"\"\"\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for shuffle_idx in range(n_shuffles):\n",
    "        # Create a shuffled version of the training data\n",
    "        shuffle_indices = np.random.RandomState(shuffle_idx).permutation(len(X_train))\n",
    "        X_shuffled = X_train[shuffle_indices]\n",
    "        y_shuffled = y_train[shuffle_indices]\n",
    "        \n",
    "        # Train model\n",
    "        model = model_class(**model_params)\n",
    "        \n",
    "        # Special handling for different model types\n",
    "        if 'random_state' in model.get_params().keys() and 'random_state' not in model_params:\n",
    "            model.set_params(random_state=42)\n",
    "        \n",
    "        model.fit(X_shuffled, y_shuffled)\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = model.predict(X_test)\n",
    "        predictions.append(preds)\n",
    "    \n",
    "    # Calculate stability metrics\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Per-sample standard deviation\n",
    "    std_per_sample = np.std(predictions, axis=0)\n",
    "    \n",
    "    # Average pairwise RMSE\n",
    "    pairwise_rmses = []\n",
    "    for i in range(n_shuffles):\n",
    "        for j in range(i+1, n_shuffles):\n",
    "            rmse = np.sqrt(np.mean((predictions[i] - predictions[j])**2))\n",
    "            pairwise_rmses.append(rmse)\n",
    "    \n",
    "    return {\n",
    "        'mean_std': np.mean(std_per_sample),\n",
    "        'max_std': np.max(std_per_sample),\n",
    "        'mean_pairwise_rmse': np.mean(pairwise_rmses),\n",
    "        'max_pairwise_rmse': np.max(pairwise_rmses),\n",
    "        'is_stable': np.max(std_per_sample) < 1e-10  # Numerical precision threshold\n",
    "    }\n",
    "\n",
    "def investigate_random_forest():\n",
    "    \"\"\"Deep dive into why Random Forest shows such high variance\"\"\"\n",
    "    print(\"\\n\\nINVESTIGATING RANDOM FOREST INSTABILITY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    X, y = make_regression(n_samples=500, n_features=10, noise=0.1, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Test different RF configurations\n",
    "    rf_configs = [\n",
    "        (\"RF (default)\", {}),\n",
    "        (\"RF (bootstrap=False)\", {'bootstrap': False}),\n",
    "        (\"RF (max_samples=1.0)\", {'max_samples': 1.0}),\n",
    "        (\"RF (n_estimators=1)\", {'n_estimators': 1}),  # Single tree\n",
    "        (\"RF (warm_start=True)\", {'warm_start': True}),\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'Configuration':<30} {'Mean RMSE':<15} {'Notes'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, extra_params in rf_configs:\n",
    "        params = {'n_estimators': 10, 'max_depth': 5, 'random_state': 42}\n",
    "        params.update(extra_params)\n",
    "        \n",
    "        try:\n",
    "            result = test_algorithm_stability(X_train, y_train, X_test, RandomForestRegressor, params, n_shuffles=5)\n",
    "            print(f\"{name:<30} {result['mean_pairwise_rmse']:<15.6f}\")\n",
    "            \n",
    "            # Let's also check what's happening with predictions\n",
    "            if name == \"RF (default)\":\n",
    "                # Get two predictions with different orderings\n",
    "                model1 = RandomForestRegressor(**params)\n",
    "                model1.fit(X_train, y_train)\n",
    "                pred1 = model1.predict(X_test[:5])\n",
    "                \n",
    "                idx = np.random.permutation(len(X_train))\n",
    "                model2 = RandomForestRegressor(**params)\n",
    "                model2.fit(X_train[idx], y_train[idx])\n",
    "                pred2 = model2.predict(X_test[:5])\n",
    "                \n",
    "                print(f\"\\n  Sample predictions comparison (first 5):\")\n",
    "                print(f\"  Original order: {pred1}\")\n",
    "                print(f\"  Shuffled order: {pred2}\")\n",
    "                print(f\"  Difference:     {pred1 - pred2}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"{name:<30} ERROR: {str(e)}\")\n",
    "\n",
    "def run_comprehensive_test():\n",
    "    \"\"\"Test multiple algorithms for row-order stability\"\"\"\n",
    "    \n",
    "    # Create dataset\n",
    "    X, y = make_regression(n_samples=2000, n_features=20, noise=0.1, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Scale features for algorithms that need it\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Define algorithms to test\n",
    "    algorithms = [\n",
    "        # Should be stable\n",
    "        (\"Linear Regression\", LinearRegression, {}, False),\n",
    "        (\"Ridge Regression\", Ridge, {'alpha': 1.0}, False),\n",
    "        (\"Decision Tree (no sampling)\", DecisionTreeRegressor, {'max_depth': 5, 'random_state': 42}, False),\n",
    "        \n",
    "        # Potentially unstable - coordinate descent\n",
    "        (\"Lasso (coord descent)\", Lasso, {'alpha': 0.1, 'random_state': 42, 'selection': 'cyclic'}, False),\n",
    "        (\"Lasso (random coord descent)\", Lasso, {'alpha': 0.1, 'random_state': 42, 'selection': 'random'}, False),\n",
    "        (\"ElasticNet\", ElasticNet, {'alpha': 0.1, 'random_state': 42}, False),\n",
    "        \n",
    "        # Potentially unstable - SGD\n",
    "        (\"SGD Regressor\", SGDRegressor, {'random_state': 42, 'max_iter': 1000}, True),\n",
    "        (\"SGD (different loss)\", SGDRegressor, {'loss': 'huber', 'random_state': 42, 'max_iter': 1000}, True),\n",
    "        (\"MLP (adam)\", MLPRegressor, {'hidden_layer_sizes': (50,), 'random_state': 42, 'max_iter': 500}, True),\n",
    "        (\"MLP (sgd)\", MLPRegressor, {'hidden_layer_sizes': (50,), 'solver': 'sgd', 'random_state': 42, 'max_iter': 500}, True),\n",
    "        (\"MLP (lbfgs)\", MLPRegressor, {'hidden_layer_sizes': (50,), 'solver': 'lbfgs', 'random_state': 42, 'max_iter': 500}, True),\n",
    "        \n",
    "        # Tree-based methods\n",
    "        (\"Random Forest\", RandomForestRegressor, {'n_estimators': 50, 'max_depth': 5, 'random_state': 42}, False),\n",
    "        (\"XGBoost (hist)\", xgb.XGBRegressor, {'n_estimators': 50, 'max_depth': 5, 'tree_method': 'hist', 'random_state': 42}, False),\n",
    "        (\"XGBoost (exact)\", xgb.XGBRegressor, {'n_estimators': 50, 'max_depth': 5, 'tree_method': 'exact', 'random_state': 42}, False),\n",
    "        (\"XGBoost (exact, no subsample)\", xgb.XGBRegressor, {'n_estimators': 50, 'max_depth': 5, 'tree_method': 'exact', 'subsample': 1.0, 'colsample_bytree': 1.0, 'random_state': 42}, False),\n",
    "        (\"LightGBM\", lgb.LGBMRegressor, {'n_estimators': 50, 'max_depth': 5, 'random_state': 42, 'verbose': -1}, False),\n",
    "        (\"LightGBM (deterministic)\", lgb.LGBMRegressor, {'n_estimators': 50, 'max_depth': 5, 'deterministic': True, 'force_row_wise': True, 'random_state': 42, 'verbose': -1}, False),\n",
    "        \n",
    "        # Other algorithms\n",
    "        (\"SVM (RBF)\", SVR, {'kernel': 'rbf', 'gamma': 'scale'}, True),  # No random_state param\n",
    "        (\"KNN\", KNeighborsRegressor, {'n_neighbors': 5}, False),\n",
    "    ]\n",
    "    \n",
    "    # Add CatBoost if available\n",
    "    if CATBOOST_AVAILABLE:\n",
    "        algorithms.append((\"CatBoost\", cb.CatBoostRegressor, {'iterations': 50, 'depth': 5, 'random_state': 42, 'verbose': False}, False))\n",
    "    \n",
    "    # Test each algorithm\n",
    "    results = []\n",
    "    \n",
    "    print(\"Testing algorithm stability under row permutation...\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Algorithm':<35} {'Mean STD':<12} {'Max STD':<12} {'Mean RMSE':<12} {'Stable?':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, model_class, params, use_scaled in algorithms:\n",
    "        X_tr = X_train_scaled if use_scaled else X_train\n",
    "        X_te = X_test_scaled if use_scaled else X_test\n",
    "        \n",
    "        try:\n",
    "            result = test_algorithm_stability(X_tr, y_train, X_te, model_class, params)\n",
    "            \n",
    "            results.append({\n",
    "                'algorithm': name,\n",
    "                **result\n",
    "            })\n",
    "            \n",
    "            stability = \"YES\" if result['is_stable'] else \"NO\"\n",
    "            print(f\"{name:<35} {result['mean_std']:<12.2e} {result['max_std']:<12.2e} {result['mean_pairwise_rmse']:<12.2e} {stability:<10}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:<35} ERROR: {str(e)[:70]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\\nSUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Categorize results\n",
    "    stable_algos = [r['algorithm'] for r in results if r['is_stable']]\n",
    "    unstable_algos = [r['algorithm'] for r in results if not r['is_stable']]\n",
    "    \n",
    "    print(f\"\\nSTABLE ALGORITHMS ({len(stable_algos)}):\")\n",
    "    for algo in stable_algos:\n",
    "        print(f\"  ✓ {algo}\")\n",
    "    \n",
    "    print(f\"\\nUNSTABLE ALGORITHMS ({len(unstable_algos)}):\")\n",
    "    for algo in unstable_algos:\n",
    "        result = next(r for r in results if r['algorithm'] == algo)\n",
    "        print(f\"  ✗ {algo} (avg RMSE between runs: {result['mean_pairwise_rmse']:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_specific_cases():\n",
    "    \"\"\"Test specific interesting cases in more detail\"\"\"\n",
    "    \n",
    "    print(\"\\n\\nDETAILED ANALYSIS OF SPECIFIC CASES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Case 1: XGBoost with different settings\n",
    "    print(\"\\n1. XGBoost with different configurations:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    xgb_configs = [\n",
    "        (\"XGB (hist, subsample=0.8)\", {'tree_method': 'hist', 'subsample': 0.8}),\n",
    "        (\"XGB (hist, subsample=1.0)\", {'tree_method': 'hist', 'subsample': 1.0}),\n",
    "        (\"XGB (exact, subsample=0.8)\", {'tree_method': 'exact', 'subsample': 0.8}),\n",
    "        (\"XGB (exact, subsample=1.0)\", {'tree_method': 'exact', 'subsample': 1.0}),\n",
    "        (\"XGB (exact, all 1.0)\", {'tree_method': 'exact', 'subsample': 1.0, 'colsample_bytree': 1.0}),\n",
    "    ]\n",
    "    \n",
    "    for name, extra_params in xgb_configs:\n",
    "        params = {'n_estimators': 50, 'max_depth': 5, 'random_state': 42}\n",
    "        params.update(extra_params)\n",
    "        \n",
    "        result = test_algorithm_stability(X_train, y_train, X_test, xgb.XGBRegressor, params, n_shuffles=5)\n",
    "        stable = \"YES\" if result['is_stable'] else \"NO\"\n",
    "        print(f\"{name:<35} RMSE: {result['mean_pairwise_rmse']:.6f}, Stable: {stable}\")\n",
    "    \n",
    "    # Case 2: Neural networks with different batch sizes\n",
    "    print(\"\\n2. Neural Networks with different batch sizes:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    mlp_configs = [\n",
    "        (\"MLP (batch_size=1000)\", {'batch_size': 1000}),  # Full batch\n",
    "        (\"MLP (batch_size=200)\", {'batch_size': 200}),    # Large batch\n",
    "        (\"MLP (batch_size=32)\", {'batch_size': 32}),      # Mini-batch\n",
    "        (\"MLP (batch_size=1)\", {'batch_size': 1}),        # SGD\n",
    "    ]\n",
    "    \n",
    "    for name, extra_params in mlp_configs:\n",
    "        params = {'hidden_layer_sizes': (50,), 'random_state': 42, 'max_iter': 200, 'learning_rate_init': 0.01}\n",
    "        params.update(extra_params)\n",
    "        \n",
    "        result = test_algorithm_stability(X_train_scaled, y_train, X_test_scaled, MLPRegressor, params, n_shuffles=5)\n",
    "        stable = \"YES\" if result['is_stable'] else \"NO\"\n",
    "        print(f\"{name:<35} RMSE: {result['mean_pairwise_rmse']:.6f}, Stable: {stable}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run comprehensive test\n",
    "    results = run_comprehensive_test()\n",
    "    \n",
    "    # Investigate Random Forest\n",
    "    investigate_random_forest()\n",
    "    \n",
    "    # Run detailed analysis\n",
    "    test_specific_cases()\n",
    "    \n",
    "    print(\"\\n\\nKEY INSIGHTS:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\"\"\n",
    "Based on the empirical results:\n",
    "\n",
    "1. **Perfectly Stable** (deterministic):\n",
    "   - Linear/Ridge regression (closed-form solutions)\n",
    "   - Decision Trees with fixed random_state\n",
    "   - Coordinate descent methods (even with 'random' selection!)\n",
    "   - KNN (no randomness involved)\n",
    "   - LightGBM (surprisingly stable by default)\n",
    "\n",
    "2. **Unstable Algorithms**:\n",
    "   - SGD-based methods (mini-batch order matters)\n",
    "   - Neural networks (batch formation depends on order)\n",
    "   - Random Forest (high variance - needs investigation)\n",
    "   - XGBoost (small but measurable differences)\n",
    "\n",
    "3. **Key Findings**:\n",
    "   - sklearn's coordinate descent properly controls randomness\n",
    "   - LightGBM appears more stable than XGBoost by default\n",
    "   - Random Forest's instability is surprisingly large\n",
    "   - Even 'exact' tree methods can be unstable with subsampling\n",
    "\n",
    "4. **To Achieve Stability**:\n",
    "   - Use algorithms with closed-form solutions when possible\n",
    "   - For XGBoost: tree_method='exact', subsample=1.0, colsample_bytree=1.0\n",
    "   - For neural networks: use full-batch training (lbfgs solver)\n",
    "   - For ensemble methods: be aware of the variance/stability trade-off\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444cf8aa",
   "metadata": {},
   "source": [
    "### Other Unsupervised Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786b407b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>mean_ari</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KMeans</td>\n",
       "      <td>0.8672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agglomerative</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GMM</td>\n",
       "      <td>0.8677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DBSCAN</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       algorithm  mean_ari\n",
       "0         KMeans    0.8672\n",
       "1  Agglomerative    1.0000\n",
       "2            GMM    0.8677\n",
       "3         DBSCAN    1.0000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Create dataset\n",
    "X, _ = make_blobs(n_samples=1000, n_features=5, centers=4, cluster_std=1.0, random_state=123)\n",
    "\n",
    "# General function\n",
    "def compute_mean_ari(X, cluster_func, n_shuffles=10, seed=42):\n",
    "    labels_list = []\n",
    "    for i in range(n_shuffles):\n",
    "        rng = np.random.RandomState(seed + i)\n",
    "        order = rng.permutation(len(X))\n",
    "        X_shuffled = X[order]\n",
    "        labels = cluster_func(X_shuffled)\n",
    "        inverse = np.argsort(order)\n",
    "        labels_unshuffled = labels[inverse]\n",
    "        labels_list.append(labels_unshuffled)\n",
    "\n",
    "    aris = [adjusted_rand_score(labels_list[i], labels_list[j])\n",
    "            for i in range(n_shuffles) for j in range(i + 1, n_shuffles)]\n",
    "    return np.mean(aris)\n",
    "\n",
    "# Define methods\n",
    "methods = {\n",
    "    \"KMeans\": lambda X: KMeans(n_clusters=4, random_state=42).fit(X).labels_,\n",
    "    \"Agglomerative\": lambda X: AgglomerativeClustering(n_clusters=4).fit(X).labels_,\n",
    "    \"GMM\": lambda X: GaussianMixture(n_components=4, random_state=42).fit(X).predict(X),\n",
    "    \"DBSCAN\": lambda X: DBSCAN(eps=1.2).fit(X).labels_,\n",
    "}\n",
    "\n",
    "# Run tests\n",
    "results = []\n",
    "for name, func in methods.items():\n",
    "    try:\n",
    "        mean_ari = compute_mean_ari(X, func)\n",
    "        results.append({\"algorithm\": name, \"mean_ari\": round(mean_ari, 4)})\n",
    "    except Exception as e:\n",
    "        results.append({\"algorithm\": name, \"mean_ari\": None, \"error\": str(e)})\n",
    "\n",
    "# Display\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ef1e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>eps</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th colspan=\"2\" halign=\"left\">0.9</th>\n",
       "      <th>1.2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blobs_hi_noise</th>\n",
       "      <td>-0.0048</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blobs_rounded</th>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0041</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>circles_noisy</th>\n",
       "      <td>-0.0010</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moons_noisy</th>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "eps                0.6     0.7     0.9            1.2\n",
       "min_samples          2       3       3      5       5\n",
       "dataset                                              \n",
       "blobs_hi_noise -0.0048  0.0017  0.0017  0.001  0.0074\n",
       "blobs_rounded  -0.0010 -0.0041  0.0050 -0.002  0.0034\n",
       "circles_noisy  -0.0010  1.0000  1.0000  1.000  1.0000\n",
       "moons_noisy    -0.0010 -0.0010  1.0000  1.000  1.0000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper: mean ARI across shuffled fits\n",
    "def mean_ari_row_shuffles(X, cluster_func, n_shuffles=10, seed=42):\n",
    "    assignments = []\n",
    "    for i in range(n_shuffles):\n",
    "        order = np.random.RandomState(seed + i).permutation(len(X))\n",
    "        labels = cluster_func(X[order])\n",
    "        # restore original row order\n",
    "        aligned = np.empty_like(labels)\n",
    "        aligned[np.argsort(order)] = labels\n",
    "        assignments.append(aligned)\n",
    "\n",
    "    return np.mean([\n",
    "        adjusted_rand_score(assignments[i], assignments[j])\n",
    "        for i in range(n_shuffles) for j in range(i + 1, n_shuffles)\n",
    "    ])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Datasets designed to stress DBSCAN\n",
    "datasets = {\n",
    "    \"blobs_hi_noise\": make_blobs(n_samples=1000, centers=4, cluster_std=3.5,\n",
    "                                 random_state=123)[0],\n",
    "    \"blobs_rounded\":  np.round(\n",
    "        make_blobs(n_samples=1000, centers=4, cluster_std=2.5,\n",
    "                   random_state=123)[0], 1),\n",
    "    \"moons_noisy\":    StandardScaler().fit_transform(\n",
    "        make_moons(n_samples=1000, noise=0.25, random_state=123)[0]),\n",
    "    \"circles_noisy\":  StandardScaler().fit_transform(\n",
    "        make_circles(n_samples=1000, factor=0.3, noise=0.18,\n",
    "                     random_state=123)[0]),\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Parameter grid to explore\n",
    "param_grid = [\n",
    "    {\"eps\": 1.2, \"min_samples\": 5},\n",
    "    {\"eps\": 0.9, \"min_samples\": 5},\n",
    "    {\"eps\": 0.9, \"min_samples\": 3},\n",
    "    {\"eps\": 0.7, \"min_samples\": 3},\n",
    "    {\"eps\": 0.6, \"min_samples\": 2},\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "rows = []\n",
    "for dname, X in datasets.items():\n",
    "    for params in param_grid:\n",
    "        eps, ms = params[\"eps\"], params[\"min_samples\"]\n",
    "\n",
    "        def run_dbscan(Z):  # closure captures eps & ms\n",
    "            return DBSCAN(eps=eps, min_samples=ms).fit(Z).labels_\n",
    "\n",
    "        ari = mean_ari_row_shuffles(X, run_dbscan)\n",
    "        rows.append({\n",
    "            \"dataset\": dname,\n",
    "            \"eps\": eps,\n",
    "            \"min_samples\": ms,\n",
    "            \"mean_ari\": round(ari, 4),\n",
    "        })\n",
    "\n",
    "results = pd.DataFrame(rows).pivot_table(\n",
    "    index=[\"dataset\"], columns=[\"eps\", \"min_samples\"], values=\"mean_ari\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ccbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
